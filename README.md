<!-- ===================== -->
<!--        PORTADA        -->
<!-- ===================== -->

#  **Prueba T√©cnica ‚Äì Ingenier√≠a de Datos**

**Autor:** Juan Camilo Barrero Vel√°squez  
**Correo:** jcbarrerov@unal.edu.co  
**Fecha:** 01/12/2025  
**Linkeln** /

---

# üìÇ √çndice

1. [Quest 1: Carga de Informaci√≥n](#Quest-1:-Carga-de-Informaci√≥n)
    - [Soluci√≥n](#Soluci√≥n)
        - [Extracci√≥n - Lectura del archivo](#Extracci√≥n---Lectura-del-archivo)
        - [Transformaci√≥n - Procesamiento de la informaci√≥n](#Transformaci√≥n---Procesamiento-de-la-informaci√≥n)
        - [Carga - Creaci√≥n del DataFrame y exportaci√≥n del documento CSV](#Carga---Creaci√≥n-del-DataFrame-y-exportaci√≥n-del-documento-CSV)
        - [Ejecucci√≥n del ETL](#Ejecucci√≥n-del-ETL)
2. [Objetivo de la Prueba](#objetivo-de-la-prueba)
3. [Arquitectura y Herramientas Usadas](#arquitectura-y-herramientas-usadas)
4. [Desarrollo y Transformaciones](#desarrollo-y-transformaciones)  
   - [Lectura del CSV](#lectura-del-csv)
   - [Limpieza de Datos](#limpieza-de-datos)
   - [Transformaciones Aplicadas](#transformaciones-aplicadas)
   - [Escritura del Resultado](#escritura-del-resultado)
5. [Conclusiones](#conclusiones)
6. [Anexos](#anexos)

---

# **Quest 1: Carga de Informaci√≥n**
Cargar un data set, realizar el cargue y depuraci√≥n del archivo OFEI1204.txt. Se debe entregar una tabla con las siguentes columnas:

| Agente | Planta | Hora_1 | Hora_2 | Hora_3 | ... | Hora_24 |
|--------|---------|---------|---------|---------|-----|-----------|

Solamente procesar los registros Tipo D. Enviar junto con la tabla resultante el c√≥digo utilizado. Explicar el paso a paso en un archivo de texto (.doc o .pdf).

## **Soluci√≥n**

Para la soluci√≥n de este punto implement√≥ de una filosof√≠a de programaci√≥n modular en la que se establecieron tres procesos principales: extracci√≥n, transformaci√≥n y carga.

### **Extracci√≥n - Lectura del archivo**

Para este proceso de lectura se dise√±√≥ la funci√≥n _read file_ que se encarga de usar la ruta del archivo a leer, `filepath`, para luego abrir el archivo utilizando un modo de lectura _"r"_ con condificaci√≥n _"utf-8"_, almacenar el contenido en la variable `content`, imprimir un mensaje de indicando el √©xito de la operaci√≥n y finalmente retornarlo

```python
def read_file(filepath: str) -> str:
    with io.open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    print("File read successfully!")
    return content
```
### **Transformaci√≥n - Procesamiento de la informaci√≥n**

En este m√≥dulo del c√≥digo se establecen dos funciones,  `extract_date` y `get_rows`.

En la funci√≥n `extract_date` se recibe el parametro `file`, que ser√° el string resultante del proceso de extracci√≥n sobre el cual se usa la expresi√≥n regular para buscar la fecha de formato "YYYY-MM-DD". En caso de encontrar la fecha imprime un mensaje de √©xito y retorna el valor de la fecha en la variable `date`, en caso de no encontrarla, retona `None`. Este dato ser√° utilizado m√°s adelante en el nombre del archivo generado.

```python
def extract_date(file:str) -> str:
    date = re.search(r'\d{4}-\d{2}-\d{2}', file)
    print("Date extracted successfully!")
    return date.group() if date else None
```

En la funci√≥n _get_rows_ se recibe nuevamente el parametro `file`, que se le asignar√° de la misma manera que en el proceso anterior. Luego, divide el texto en bloques usando el patr√≥n "\n\n\nAGENTE:" presente en el texto. Almacena el nombre del agente que estar√° presente en las filas venideras correspondientes al bloque e itera por las lineas del bloque.

En la iteraci√≥n intenta dividir la informaci√≥n de la fila en las respectivas columnas usando la coma como patr√≥n, si el segundo elemento es " D" entonces almanecena la fila en la variable `complete_line` teniendo como primer elemento el agente y lo agrega a la lista `rows` luego de dividir cada elemento de la cadena por comas. `rows` es por lo tanto una lista de listas donde cada elemento dentro de la lista principal es la lista correspondiente a la informaci√≥n de una fila con sus elementos (que son cadenas de texto) separados.

Todo lo descrito en el parrafo anterior se encuentra dentro de un `try`, que en caso de un `IndexError` se encargar√° de imprimir una advertencia en consola estableciendo la linea que ha de ser excluida y continuando la iteraci√≥n. Todo esto, con el fin de poder hacer una revisi√≥n completa de la cadena.

Finalmente la funci√≥n imprime un mensaje de √©xito y retorna la variable `rows` con las filas que cumplen la condici√≥n establecida. 

```python
def get_rows(file:str) -> List[List[str]]:
    rows = []
    for block in file.split("\n\n\nAGENTE: "):
        agente = block.split("\n")[0]

        for line in block.split("\n")[1:]:
            try:
                if line.split(",")[1] == " D":
                    complete_line = [agente + " ," + line]
                    rows.append(complete_line[0].split(","))
            except IndexError:
                print(f"WARNING! line being excepted:'{line}'")
    print("Rows obtained successfully!")
    return rows
```
### **Carga - Creaci√≥n del DataFrame y exportaci√≥n del documento CSV**

Este m√≥dulo est√° compuesto de dos funciones `create_dataframe` y `load_csv`.

En la funci√≥n `create_dataframe` recibe los parametros rows (una lista de listas de strings) resultante del m√≥dulo de extracci√≥n y columns correspondiente a una lista que contiene los caracteres correspondientes a los nombres de las columnas.

Luego se crea un dataFrame de pandas con las filas y columnas establecidas en los par√°mentros, utiliza el metodo map que permite retirar los espacios extra al inicio y fin de cada valor si es un string y finalmente imprime un mensaje estableciendo el √©xito de la ejecuci√≥n y retorna el dataFrame.

```python
def create_dataframe(rows:List[List[str]], columns:List[str]) -> pd.DataFrame:
    df = pd.DataFrame(rows, columns=columns)
    df = df.map(lambda x: x.strip() if isinstance(x, str) else x)
    print("DataFrame created successfully!")
    return df
```

La funci√≥n `load_csv` tiene los par√°metros `df` (el dataFrame retornado en la funci√≥n anterior), `date` (la fecha obtenida en la funci√≥n `extract_date`) y `path` (la ruta donde el archivo va a ser guardado). En este caso el path no es un par√°metro obligatorio, de ser especificado lo usa para guardar el archivo dentro del directorio.

Esta funci√≥n genera el archivo CSV sin √≠ndices con el nombre `"OFFEI_cleansed_{date}.csv"`, donde `{date}` es reemplazado por la fecha. Finalmente imprime un mensaje indicando que el archivo se gener√≥ correctamente. 

```python
def load_csv(df:pd.DataFrame, date:str, path:str | None = None) -> None:
    if path == None:
        path_csv = f"OFFEI_cleansed_{date}.csv"
    else: 
        path_csv = f"{path}/OFFEI_cleansed_{date}.csv" 
    df.to_csv(path_csv, index=False)
    print("CSV file loaded successfully!")
```
### **Ejecucci√≥n del ETL**

Finalmente se realiza la importac√≠on de las funciones de los m√≥dulos y se establece la ruta d√≥nde se encuentra el archivo. Se llaman las funciones con los argumentos correspondientes y asignamos el retorno de las funciones a las variables necesarias para seguir el proceso de ETL

```python
import os

from module.extract import read_file
from module.transform import extract_date, get_rows
from module.load import create_dataframe, load_csv

PATH = os.path.abspath("../data/OFEI1204.txt")

if __name__ == '__main__':

    file = read_file(PATH)
    date = extract_date(file)
    rows = get_rows(file)
    columns = ["Agente", "Planta", "Tipo"] + ["Hora_{}".format(i) for i in range(1, 25)]
    df = create_dataframe(rows, columns)
    load_csv(df,date)
```

### **Resultados**

![Prueba de ejecucion](quest1/img/diagrama.png)



---

# **Quest 2: Manipulaci√≥n de datos**

1. Cargar un data set, del archivo Excel Master Data, √∫nicamente las siguientes
columnas:
    - Nombre visible Agente
    - AGENTE (OFEI)
    - CENTRAL (dDEC, dSEGDES, dPRU‚Ä¶)
    - Tipo de central (Hidro, Termo, Filo, Menor)

2. Seleccionar los registros que pertenecen al agente EMGESA √≥ EMGESA S.A. y adicionalmente que el Tipo de Central sea ‚ÄòH‚Äô o ‚ÄòT‚Äô.
3. Cargar el archivo dDEC1204.TXT que viene por Central.
4. Realizar el merge de los dos data sets por Central.
5. Calcular la suma horizontal de todas las horas para cada planta.
6. Seleccionar solamente los registros de las plantas cuya suma horizontal sea mayor que cero.
7. Los resultados deben ser entregados en un dataset.
8. Enviar junto con la tabla resultante el c√≥digo utilizado.
9. Explicar el paso a paso en un archivo de texto (.doc o .pdf).

## **Soluci√≥n**

Para trabajar este problema se utiliz√≥ la ayuda de _Jupyter notebooks_ debido a su facilidad para seguir el flujo de las tareas solicitadas y visualizar los dataframes. Para este programa usaremos la librer√≠a _os_, para poder interpretar las rutas relativas de los archivos, y _pandas_ para trabajar con los datos tabulares. Adicionalmente definimos las rutas relativas de los archivos necesarios para el c√≥digo.

```python
import os
import pandas as pd

PATH_EXCEL = os.path.abspath("../data/Datos Maestros VF.xlsx")
PATH_TXT = os.path.abspath("../data/dDEC1204.TXT")
PATH_TO_SAVE = os.path.abspath("./Dataset.csv") 
```

### **1. Cargar el data set**

Para cargar el dataset se utiliz√≥ el metodo `read_excel` de pandas en modo binario `'rb'` especificando en nombre de la hoja que contiene la informaci√≥n y el DataFrame resultante se asign√≥ a la variable `df_excel_raw`. 

Adicionalmente se cre√≥ un diccionario que contiene pares clave-valor con los nombres de las columnas del DataFrame y nombres simplificados sin espacio respectivamente para posteriormente renombrar las columnas.

Utilizando un `for` que recorre cada par del diccionario se renombran las columnas del DataFrame `df_excel_raw` sin necesidad de crear un nuevo DataFrame gracias al `inplace=True`.

Luego, se seleccionan los datos del DataFrame de las columnas:
 - Nombre visible Agente que es `AGENTE_VISIBLE`
 - AGENTE (OFEI) que es `AGENTE_OFEI`
 - CENTRAL (dDEC, dSEGDES, dPRU‚Ä¶) que es `CENTRAL`
 - Tipo de central (Hidro, Termo, Filo, Menor) que es `TIPO_CENTRAL`

utilizando la variable `select_columns_excel` que contiene los valores de las columnas a seleccionar del DataFrame se realiza la selecci√≥n de los valores del DataFrame `df_excel_raw` creando una copia. El nuevo DataFrame es asignado a la variable `df_excel_selected`.

```python
df_excel_raw = pd.read_excel(open(PATH_EXCEL, 'rb'),
              sheet_name='Master Data Oficial')

dic_columns = { 'Nombre visible Agente':'AGENTE_VISIBLE'
                ,'AGENTE (OFEI)':'AGENTE_OFEI'
                ,'CENTRAL (dDEC, dSEGDES, dPRU‚Ä¶)':'CENTRAL'
                ,'Tipo de central (Hidro, Termo, Filo, Menor)':'TIPO_CENTRAL'}

for name, rename in dic_columns.items():
    df_excel_raw.rename(columns={name: rename}, inplace=True)

select_columns_excel = list(dic_columns.values())
df_excel_selected = df_excel_raw[select_columns_excel].copy()
```

### **2. Filtrar por `AGENTE_VISIBLE` y `CENTRAL`**

Para esta secci√≥n se filtr√≥ el DataFrame df_excel_selected por tres condiciones:
1. `['AGENTE_VISIBLE'] == "EMGESA"` Selecciona filas donde el agente tiene nombre visible "EMGESA".
2. `['AGENTE_OFEI'] == "EMGESA S.A."` Selecciona filas donde el `AGENTE_OFEI` coincide con "EMGESA S.A.".
3. `['TIPO_CENTRAL'].isin(['H', 'T'])` Filtra solo los registros cuyo tipo de central sea H o T utilizando `isin` y la lista con los valores deseados.

La condici√≥n 1 y 2 est√°n vinculadas por una condic√≥n `or`, mientras que estas dos est√°n ligadas por una condici√≥n `and` con la 3. Finalmente El DataFrame resultate se asigna a la variable `df_excel_filtered`.

```python
df_excel_filtered = df_excel_selected[((df_excel_selected['AGENTE_VISIBLE'] == "EMGESA")
                                        | (df_excel_selected['AGENTE_OFEI'] == "EMGESA S.A."))
                                        & (df_excel_selected['TIPO_CENTRAL'].isin(['H', 'T']))]
```

### **3. Cargar el archivo dDEC1204.TXT**

Para cargar el archivo dDEC1204.TXT que contine los datos por central se cre√≥ primero una lista con las columnas correspondientes a el archivo a cargar haciendo uso de un `for` para crear los 24 elementos correspondientes a las horas, luego, se utiliz√≥ el metodo `read_csv` de pandas para leer el archivo utilizando `encoding="latin1"` y se asign√≥ al DataFrame `df_text`. Finalmente, se asignaron las columnas almacenadas en la lista columns al DataFrame.

```python
columns = ["CENTRAL"] + ["Hora_{}".format(i) for i in range(1, 25)]
df_text = pd.read_csv(PATH_TXT, encoding="latin1")
df_text.columns = columns
```

### **4. Realizar el merge de los dos data sets por `CENTRAL`**

Para realizar el merge de los DataFrames utilizamos el m√©todo de pandas `merge` que nos permite hacer una uni√≥n de dos DataFrames. Establecemos el par√°metro `on="CENTRAL"` que indica que la uni√≥n se hace por la columna "CENTRAL" presente en ambos DataFrames y establecemos que el tipo de uni√≥n como `how="left"` que es similar al de un _left join_ en el cual se toman todas las filas de `df_excel_filtered` y solamente se agregan los valores de las filas correspondientes de `df_text` si el valor de la columna "CENTRAL" coincide. De esta manera se asegura de que no se pierden valores de `df_excel_filtered` en caso de que no haya coincidencia. Finalmente se asigna el DataFrame resultante a `df_merged`.

```python
df_merged = pd.merge(df_excel_filtered, df_text, on="CENTRAL", how="left")
```

### **5. Calcular la suma horizontal de todas las horas para cada planta**

Para realizar la suma horizontal se cre√≥ la variable tipo lista `columns_to_sum` que almacena los valores de las columnas que queremos sumar (en este caso las correspondientes a las 24 horas). A continuaci√≥n, utilizamos `df_merged[columns_to_sum]` para seleccionar las columnas y aplicamos el metodo `sum(axis=1)` para realizar la suma, donde `axis=1` establece que la suma debe ser realizada a lo largo de las filas (suma horizontal). Finalmente, el vector resultante es almacenado en la columna "SUM_OF_HOURS" que se asigna dentro del m√≠smo dataframe `df_merged`.

```python
columns_to_sum = ["Hora_{}".format(i) for i in range(1, 25)]
df_merged["SUM_OF_HOURS"]=df_merged[columns_to_sum].sum(axis=1)
```

### **6. Seleccionar los registros de las plantas con suma horizontal mayor que cero**

Para seleccionar los registros se utiliz√≥ la condicion `df_merged["SUM_OF_HOURS"] > 0` que nos otorga un vector de booleanos que establecen si se cumple la condici√≥n, que la columna "SUM_OF_HOURS" sea mayor que cero. Luego, se inserta el vector dentro del DataFrame `df_merged` para seleccionar las filas que cumplen la condici√≥n. Por √∫ltimo, se asign√≥ el DataFrame a la variable `df_final`. 

```python
df_final = df_merged[df_merged["SUM_OF_HOURS"] > 0]
```

### **7. Cargar los resultados en un Data set**

Para finalizar este ejercicio se utiliz√≥ el metodo `to_csv` para cargar el DataFrame en un archivo de valores separados por comas utilizando la variable `PATH_TO_SAVE` establecida al inicio del c√≥digo como ruta.

```python
df_final.to_csv(PATH_TO_SAVE)
```

# **Quest 3: Prueba de SQL**

Utiliza cualquier dialecto de SQL de tu elecci√≥n para abordar estos desaf√≠os, de preferencia genera los datos si lo ves necesario para simular y emplear las soluciones de dise√±o, la idea es explicar tu soluci√≥n de tal forma que t√©cnicamente el equipo pueda ser capaz de entender y visualizar usa las herramientas que desees adem√°s de hacer los scripts de creaci√≥n dependiendo de cada parte de la prueba.

**Parte 1** Un interesado nos solicita prepararnos para una nueva fuente de datos dentro de nuestro entorno de almacenamiento de datos. La tabla recoger√° informaci√≥n meteorol√≥gica de forma horaria para diferentes regiones. Las dimensiones y m√©tricas de la tabla deben crearse con los tipos de datos apropiados. La tabla debe dise√±arse de manera que se pueda identificar de forma √∫nica cada registro dentro de ella. Las siguientes columnas deben estar presentes en la definici√≥n de la tabla:
- Localidad (Poblados en Medell√≠n, Envigado, Sabaneta, etc.)
- Pa√≠s (Colombia)
- Temperatura (Grados Celsius)
- Fecha y hora del registro (horario)
- Cobertura de nubes (M√≠nima, Parcial, Total)
- √çndice U/V
- Presi√≥n atmosf√©rica
- Velocidad del viento (Nudos)

**Parte 2** La tabla definida en la Parte 1 se implementa y comienza a recopilar datos. La tabla se vuelve considerablemente grande, con millones de registros. Proporciona tres maneras en que la tabla actual puede mejorarse para manejar un conjunto de datos m√°s grande y mantener una √≥ptima legibilidad de los datos.

**Parte 3** El mismo interesado llega con nuevos requerimientos. Adem√°s de la tabla ya existente, se requiere una nueva tabla completamente separada que recopile la misma informaci√≥n, pero registre la temperatura en Fahrenheit (en lugar de Grados Celsius). Adem√°s, la nueva tabla contendr√° los registros de temperatura distribuidos por d√≠a, en lugar de por hora. La nueva tabla debe contener todos los datos ya recopilados de la tabla definida en la Parte 1.

**Parte 4** Se recibe un nuevo requerimiento por parte del interesado. Ambas tablas definidas en la Parte 1 y la Parte 3 deben ahora capturar la diferencia de temperatura (delta) entre un registro y el anterior. En el caso horario, la nueva m√©trica contendr√° la diferencia entre el momento actual y una hora antes. Para el caso diario, la nueva m√©trica contendr√° la diferencia entre el momento actual y el d√≠a anterior. La nueva columna debe ser completada retroactivamente para todas las temperaturas ya existentes.

## **Soluci√≥n**

Para la realizaci√≥n de este punto se utiliz√≥ SQL Server como dialecto y desplegamos la base de datos mediante el uso de docker desktop con una imagen de SQL Server 2022. Se realiz√≥ una conexi√≥n de la base de datos con el programa Azure Data Studio y se cre√≥ la base de datos "WEATHER".

### **Parte 1. Creaci√≥n de la tabla `CLIMA` y carga**

Para la creaci√≥n de la tabla `CLIMA` se decidi√≥ utilizar un `ID` como `PRIMARY KEY` que es un entero que se inserta autom√°ticamente asignando un valor que incrementa en 1 con cada `INSERT` que se realiza gracias a `IDENTITY(1,1)`, esto permite identificar de manera √∫nica cada dato. 

Para los datos relacionados con nombres de locaciones, y caracter√≠sticas clim√°ticas (`LOCALIDAD`, `PAIS` y `COVERTURA`) se utilizo el tipo de dato VARCHAR ya que ninguno tiene una longitud definida de caracteres y se ha definido el m√°ximo de caracteres basado en qu√© tan largo podr√≠an ser los nombres a insertar.

Para las mediciones meteorol√≥gicas (`TEMP_CELCIUS`, `COVERTURA`, `INDICE_UV`, `PRESI√ìN_ATM` y `VEL_VIENTO_NUDOS`) se estableciero como `DECIMAL` con precisiones y escalas adecuadas para los tipos de mediciones. Por ejemplo, el `INDICE_UV` mayor a 11 es muy alto y en ocasiones puede ser registrado como un n√∫mero decimal, en este caso podr√≠a ser almacenado con una precisi√≥n de dos d√©cimas.

Para el dato `FECHA_HORA` se estableci√≥ el tipo de dato `DATETIME2` que puede almacenar la fecha en formato `YYYY-MM-DD hh:mm:ss[.fracci√≥n]` que puede almacenar perfectamente datos como `2023-02-27 13:45:20` sin necesidad de especificar la fracci√≥n.

Para que cada valor tenga significado se realiz√≥ una restricci√≥n en la cu√°l la pareja `LOCALIDAD` y `FECHA_HORA` son √∫nicas ya que no puede haber dos climas y condiciones meteorol√≥gicas iguales al mismo momento en el mismo lugar. Ninguno de los valores a ingresar podr√° ser nulo.

```sql
CREATE TABLE CLIMA (
    ID INT IDENTITY(1,1) PRIMARY KEY,
    LOCALIDAD VARCHAR(150) NOT NULL,
    PAIS VARCHAR(150) NOT NULL,
    TEMP_CELCIUS DECIMAL(5,2) NOT NULL,
    FECHA_HORA DATETIME2 NOT NULL,
    COVERTURA VARCHAR(50) NOT NULL,
    INDICE_UV DECIMAL(4,2) NOT NULL,
    PRESION_ATM DECIMAL(6,2) NOT NULL,
    VEL_VIENTO_NUDOS DECIMAL(6,2) NOT NULL,
    
    CONSTRAINT LOCALIDAD_FECHA UNIQUE (LOCALIDAD, FECHA_HORA)
);
```

Debido al corto tiempo y para poder tener valores en la base de datos se pidi√≥ a una chat de texto predictivo generar un c√≥digo en python para poder generar valores aleatorios autom√°ticamente. Se realiz√≥ una conexi√≥n con la base de datos y se insertaron utilizando la librer√≠a `sqlalchemy` de python. Por favor revisar `generate_data.py`.

### **Parte 2. Tres mejoras para mejorar la lectura con tablas grandes**

#### **Implementaci√≥n de Particionamiento**

Las tablas grandes pueden ser particionadas para mejorar la consulta, en este caso, la tabla podr√≠a particionarse por un rango de fechas. Si la tabla `CLIMA` se particiona cada mes o a√±o, por ejemplo, se generar√≠an bloques de datos mucho m√°s peque√±os. De esta mandera las consultas que incluyen `WHERE FECHA_HORA BETWEEN` se convierten en consultas m√°s eficientes ya que no tienen que leer todos los millones de datos y se hace un uso de los √≠ndices m√°s eficiente.

#### **Implementaci√≥n de √≠ndices m√°s elaborados**

La creaci√≥n de indices adecuados para la consulta puede mejorar considerablemente la velocidad de la misma, de igual manera, los indices mal dise√±ados pueden no apoyar la consulta de la informaci√≥n o incluso empeorar el proceso al demandar m√°s almacenamiento y procesamiento. Un √≠ndice que se podr√≠a implementar es el de la columna `FECHA_HORA`, inculsive, si las consultas son recurrentes para un lugar y tiempo espec√≠fico se podr√≠a crear un √≠ndice compuesto (`LOCALIDAD`, `FECHA_HORA`). La creaci√≥n de los √≠ndices necesarios para mejorar la consulta depender√° del prop√≥sito de las consultas y las consultas que se realicen de manera frecuente en la base de datos. 

#### **Normalizaci√≥n de los datos en un modelado estrella**

Existen muchos datos que pueden llegar a ser redundantes cuando la informaci√≥n se almacena a gran escala. Por ejemplo, si la informaci√≥n recopilida del pa√≠s siemepre es la m√≠sma o var√≠a muy poco al igual que las localidades puede crearse una tabla de dimensiones y de hechos para mejorar el almacenamiento y consulta. Por ejemplo, se podr√≠an crear tablas de dimensi√≥n de localidad, tiempo y clima(refiriendonos a la variable de covertura) en torno a una tabala de hechos que contiene las m√©tricas del clima. 

### **Parte 3. Creaci√≥n de la tabla `CLIMA_DIA` y cargue de los datos**

Para la creaci√≥n de la tabla `CLIMA_DIA` se utiliz√≥ un formato de datos similar al utilizado para la tabla `CLIMA` para las m√©tricas ya existentes se crearon las columnas con el prefijo `AVG` _average_ en ingl√©s para la palabra promedio, ya que los datos insertados a esta tabla tendr√°n el promedio de los valores registrados durante el d√≠a en la fecha especificada. Ya que el formato de la columna `FECHA` solo necesita almacenar `YYYY-MM-DD` podemos cambiar el tipo de dato a almacenar a `DATE`.

La columna `AVG_TEMP_FAHRENHEIT` almacenar√° el promedio de las temperaturas registradas en el d√≠a en escala Fahrenheit. Parar la columna `SET_COVERTURA` se almacenar√° una cadena de texto con los climas registrados durante el d√≠a, debido a que no se conoce cuantos caracteres contendr√° se estableci√≥ como tipo de dato `VARCHAR(MAX)`. El `PRIMARY KEY` se estableci√≥ de la misma manera en como se hizo para la tabla `CLIMA` y nuevamente se cre√≥ un `CONSTRAINT` para que el par `LOCALIDAD`, `FECHA` fueran √∫nicos, nuevamente ninguno de los valores a ingresar podr√° ser nulo.

```sql
CREATE TABLE CLIMA_DIA (

    ID INT IDENTITY(1,1) PRIMARY KEY,
    LOCALIDAD VARCHAR(150) NOT NULL,
    PAIS VARCHAR(150) NOT NULL,
    AVG_TEMP_FAHRENHEIT DECIMAL(5,2) NOT NULL,
    FECHA DATE NOT NULL, 
    SET_COVERTURA VARCHAR(MAX) NOT NULL,
    AVG_INDICE_UV DECIMAL(4,2) NOT NULL,
    AVG_PRESION_ATM DECIMAL(6,2) NOT NULL,
    AVG_VEL_VIENTO_NUDOS DECIMAL(6,2) NOT NULL,
    
    CONSTRAINT LOCALIDAD_FECHA_DIA UNIQUE (LOCALIDAD, FECHA)
);
```

Para la inserci√≥n de los datos se utiliz√≥ el siguiente c√≥digo podemos explicar el c√≥digo por secciones:

#### **CTE para consultar los valores agregados**

En esta secci√≥n de c√≥digo se utiliza la consulta temporal `CTE1`, las funciones de agregaci√≥n y agrupaci√≥n para obtener los datos solicitados que ser√°n insertados en la tabla `CLIMA_DIA`. 

La funci√≥n de agregaci√≥n por excelencia en esta consuta es `AVG` que se encarga de calcular el promedio de los valores seleccionados por el GROUP BY. Un aspecto relevante en la funci√≥n de agregaci√≥n usada en la m√©trica de temperacuta es que se utiliza la funci√≥n sobre `((a.TEMP_CELCIUS * 9.0/5.0) + 32)` ya que la temperatura de la tabla `CLIMA` est√° expresada en grados celcius. 

En el caso de la `FECHA` se utiliza la funci√≥n `CAST`, que convierte el tipo de dato que se ten√≠a en la tabla `CLIMA` (`DATETIME2`) a `DATE` para que √∫nicamente selecione la fecha con el formato establecido en la tabla CLIMA_DIA. Para la columna `SET_COVERTURA` se utiliz√≥ la funci√≥n `STRING_AGG` que se encarga de agrupar todos los valores seleccionados por el `GROUP BY` y separarlos por `', '`. Adicionalmente, a cada columna se le asigna el alias correspondiente al nombre de la columna de la tabla `CLIMA D√çA`.

Por √∫ltimo, se utiliza el `GROUP BY` por `LOCALIDAD`, `PAIS` y `CAST(a.FECHA_HORA AS DATE)`, de esta manera las agregaciones ser√°n aplicadas diariamente para cada localidad independientemente.

#### **Inserci√≥n a la tabla `CLIMA_DIA`**

Luego se realiza la inserci√≥n de los datos resultantes de la `CTE1`, insertando as√≠ cada fila como un registro diario y utilizando la estructura de las columnas de la tabla `CLIMA_DIA`.

```sql
WITH CTE1 AS (
    SELECT
        a.LOCALIDAD                                 AS LOCALIDAD,
        a.PAIS                                      AS PAIS,
        CAST(a.FECHA_HORA AS DATE)                  AS FECHA,
        AVG((a.TEMP_CELCIUS * 9.0/5.0) + 32)        AS AVG_TEMP_FAHRENHEIT,
        STRING_AGG(a.COVERTURA, ', ')               AS SET_COVERTURA,
        AVG(a.INDICE_UV)                            AS AVG_INDICE_UV,
        AVG(a.PRESION_ATM)                          AS AVG_PRESION_ATM,
        AVG(a.VEL_VIENTO_NUDOS)                     AS AVG_VEL_VIENTO_NUDOS
    FROM WEATHER.dbo.CLIMA AS a
    GROUP BY
        a.LOCALIDAD,
        a.PAIS,
        CAST(a.FECHA_HORA AS DATE)
)

INSERT INTO WEATHER.dbo.CLIMA_DIA (
    LOCALIDAD,
    PAIS,
    FECHA,
    AVG_TEMP_FAHRENHEIT,
    SET_COVERTURA,
    AVG_INDICE_UV,
    AVG_PRESION_ATM,
    AVG_VEL_VIENTO_NUDOS
)
SELECT
    LOCALIDAD,
    PAIS,
    FECHA,
    AVG_TEMP_FAHRENHEIT,
    SET_COVERTURA,
    AVG_INDICE_UV,
    AVG_PRESION_ATM,
    AVG_VEL_VIENTO_NUDOS
FROM CTE1;
```
Como adici√≥n a esta parte del punto en el c√≥digo se a√±adi√≥ el c√≥digo referentet a un procedimiento almacenado, que permitir√≠a realizar la carga de datos a la tabla `CLIMA_DIA` estableciendo la variable `@FECHA` para un d√≠a espec√≠fico deseado. Por favor revisar el c√≥digo.

### **Parte 4. Uso de funciones de ventana para calcular las diferencias de temperatura**

Como primer paso para resolver este punto se a√±adi√≥ una nueva columna a la tabla `CLIMA` llamada `DELTA_TEMP_C`, Esta columna tiene el mismo tipo de dato que `TEMP_CELCUIUS` y no podr√° ser nulo.

Paso seguido se utiliz√≥ una consulta temporal `CTE` para obtener los datos con los que posteriormente se calcular√° la diferencia de temperatura. En esta consulta utilizamos la funci√≥n `LAG(TEMP_CELCIUS, 1)` que devuelve el valor previo a `TEMP_CELCIUS` en la fila inmediatamente anterior sobre la partici√≥n seleccionada. En la partici√≥n se selecci√≥nan bloques por `LOCALIDAD` y `PAIS` y se ordenan por `FECHA_HORA` de manera ascendente. De esta manera la partici√≥n ser√° para la misma `LOCALIDAD`, perteneciente al m√≠smo `PAIS` ordena da por `FECHA_HORA` en la cual los datos est√°n ordenados en orden de recolecci√≥n, as√≠ la funci√≥n `LAG` devolver√° el dato de la hora inmediatamente anterior a la fila en la que se encuentra, cabe aclarar que para el primer valor sobre la partici√≥n el resultado ser√° `NULL` (ya que no existe un valor anterior a este). A esta funci√≥n `LAG` se le as√≠gna el nombre de `TEMP_PREVIA`.

Por √∫ltimo, la secci√≥n de UPDATE se encarga de realizar un `INNER JOIN` por `ID` de la `CTE` con la tabla CLIMA para posteriormente calcular la diferencia de temperaturas y hacer el `UPDATE` de la columna `DELTA_TEMP_C` con el c√°lculo. 

```sql
ALTER TABLE WEATHER.dbo.CLIMA ADD 
    DELTA_TEMP_C DECIMAL(5,2) NULL;

WITH CTE AS (
    SELECT
        ID,
        TEMP_CELCIUS,
        LAG(TEMP_CELCIUS, 1) OVER (
            PARTITION BY LOCALIDAD, PAIS
            ORDER BY FECHA_HORA
        ) AS TEMP_PREVIA
    FROM WEATHER.dbo.CLIMA
)

UPDATE C
SET C.DELTA_TEMP_C = C.TEMP_CELCIUS - T.TEMP_PREVIA
FROM WEATHER.dbo.CLIMA AS C
INNER JOIN CTE AS T
    ON C.ID = T.ID;
```
Para la tabla `CLIMA_DIA` se realiza el m√≠smo porcedimiento que con la anterior, la columna a√±adida se llam√≥ `DELTA_TEMP_F`.

```sql
ALTER TABLE WEATHER.dbo.CLIMA_DIA ADD 
    DELTA_TEMP_F DECIMAL(5,2) NULL;

WITH CTE AS (
    SELECT
        ID,
        AVG_TEMP_FAHRENHEIT,
        LAG(AVG_TEMP_FAHRENHEIT, 1) OVER (
            PARTITION BY LOCALIDAD, PAIS
            ORDER BY FECHA
        ) AS TEMP_PREVIA
    FROM WEATHER.dbo.CLIMA_DIA
)
UPDATE C
SET C.DELTA_TEMP_F = C.AVG_TEMP_FAHRENHEIT - T.TEMP_PREVIA
FROM WEATHER.dbo.CLIMA_DIA AS C
INNER JOIN CTE AS T
    ON C.ID = T.ID;
```
---

# **Quest 5: Prueba Azure**

Construya una soluci√≥n completa en la nube de Azure que usando todas la base, de pruebas adventure Works, permita crear una etl, para la realizaci√≥n de un trabajo de reporteria dentro de la organizaci√≥n.
 - desplegar base de datos en sql, con la base de pruebas adventure works
 - realizar un pipeline con Azure Datafactory, utilizando data flow, para realizar la carga de una base de datos, crear 5 indicadores.
 - realizar una etl, que poble un datalake.

## **Soluci√≥n**

En esta prueba t√©cnica se plante√≥ la siguiente arquitectura:
![Arquitectura Propuesta](quest5\img\arquitectura.png)

El prop√≥sito de este esquema es plantear una arquitectura sencilla para desplegar los servicio b√°sicos, se omitieron pr√°cticas detalladas para la asignaci√≥n de nombres y recursos de red. Pero se hizo el ejercicio de repartir grupos de recurso por contexto. En este caso, c√≥mputo, almacenamiento y network.

### **Data Factory**

Se despleg√≥ el recurso de Data Factory, se hicieron las correspondientes conexiones entre los servicios vinculados 

![Data Factory](quest5\img\)

### **KPIs para la base de datos de Adventure Works**

A continuaci√≥n, se presentan las siguientes consultas que representan la l√≥gica de los KPIs, estos KPIs fueron extra√≠dos desde Github:

#### **1. KPI Ventas Totales**

Este KPI representa la suma del importe facturado por las l√≠nas de pedido o _line totals_ por periodo.

```sql
SELECT
  DATEPART(YEAR, h.OrderDate) AS Year,
  DATEPART(MONTH, h.OrderDate) AS Month,
  SUM(d.LineTotal) AS TotalSales
FROM SalesLT.SalesOrderHeader h
JOIN SalesLT.SalesOrderDetail d ON h.SalesOrderID = d.SalesOrderID
WHERE h.OrderDate BETWEEN '2000-01-01' AND '2025-12-31'
GROUP BY DATEPART(YEAR,h.OrderDate), DATEPART(MONTH,h.OrderDate);
```
#### **2. KPI Margen Bruto Porcentaje**

Representa la rentabilidad de los gastos operativos por periodo.

```sql
SELECT
  YEAR(h.OrderDate) AS Year,
  MONTH(h.OrderDate) AS Month,
  SUM(d.LineTotal) AS Revenue,
  SUM(p.StandardCost * d.OrderQty) AS COGS,
  CASE WHEN SUM(d.LineTotal) = 0 THEN NULL
       ELSE (SUM(d.LineTotal) - SUM(p.StandardCost * d.OrderQty)) * 100.0 / SUM(d.LineTotal)
  END AS GrossMarginPct
FROM SalesLT.SalesOrderHeader h
JOIN SalesLT.SalesOrderDetail d ON h.SalesOrderID = d.SalesOrderID
JOIN SalesLT.Product p ON d.ProductID = p.ProductID
WHERE h.OrderDate BETWEEN '2000-01-01' AND '2025-12-31'
GROUP BY YEAR(h.OrderDate), MONTH(h.OrderDate);
```

#### **KPI 3. Valor Promedio Por Pedido**

Promedio del importe por √≥rdenes de un periodo.

```sql
WITH OrderTotals AS (
  SELECT d.SalesOrderID, SUM(d.LineTotal) AS OrderTotal, MIN(h.OrderDate) AS OrderDate
  FROM SalesLT.SalesOrderHeader h
  JOIN SalesLT.SalesOrderDetail d ON h.SalesOrderID = d.SalesOrderID
  WHERE h.OrderDate BETWEEN '2000-01-01' AND '2025-12-31'
  GROUP BY d.SalesOrderID
)
SELECT YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month,
       AVG(OrderTotal) AS AOV, COUNT(*) AS OrdersCount
FROM OrderTotals
GROUP BY YEAR(OrderDate), MONTH(OrderDate);
```

#### **KPI 4. Tasa entregas a tiempo**

Porcentaje de √≥rdenes por linea enviadas en la fecha o antes de la fecha estipulada.

```sql
SELECT YEAR(h.ShipDate) AS Year, MONTH(h.ShipDate) AS Month,
       SUM(CASE WHEN h.ShipDate <= h.DueDate THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS OnTimePct,
       COUNT(*) AS TotalShipments
FROM SalesLT.SalesOrderHeader h
WHERE h.ShipDate IS NOT NULL AND h.ShipDate BETWEEN '2000-01-01' AND '2025-12-31'
GROUP BY YEAR(h.ShipDate), MONTH(h.ShipDate);
```

#### **KPI 5. Top 5 Productos Mes**
.
Representa el top 5 de los productos m√°s vendidos por mes 

```sql
WITH MonthlyProductSales AS (
    SELECT 
        DATEPART(YEAR, h.OrderDate) AS A√±o,
        DATEPART(MONTH, h.OrderDate) AS Mes,
        p.Name AS Producto,
        SUM(d.LineTotal) AS VentasProducto,
        RANK() OVER (
            PARTITION BY DATEPART(YEAR, h.OrderDate),
                         DATEPART(MONTH, h.OrderDate)
            ORDER BY SUM(d.LineTotal) DESC
        ) AS RankingMensual
    FROM SalesLT.SalesOrderHeader h
    JOIN SalesLT.SalesOrderDetail d 
        ON h.SalesOrderID = d.SalesOrderID
    JOIN SalesLT.Product p
        ON d.ProductID = p.ProductID
    GROUP BY 
        DATEPART(YEAR, h.OrderDate),
        DATEPART(MONTH, h.OrderDate),
        p.Name
)
SELECT *
FROM MonthlyProductSales
WHERE RankingMensual <= 5
ORDER BY A√±o, Mes, RankingMensual;
```

### **DataFlows**

A continucaci√≥n se presentan los DataFLows creados para cada KPI:

#### **1. KPI Ventas Totales**

![1. KPI Ventas Totales](quest5\img\DFVentasTotales.png)

#### **2. KPI Margen Bruto Porcentaje**

![2. KPI Margen Bruto Porcentaje](quest5\img\DFMargenBrutoPorcentaje.png)

#### **KPI 3. Valor Promedio Por Pedido**

![KPI 3. Valor Promedio Por Pedido](quest5\img\DFValorPromedioPorPedido.png)

#### **KPI 4. Tasa entregas a tiempo**

![KPI 4. Tasa entregas a tiempo](quest5\img\DFTasaEntregasaTiempo.png)

#### **KPI 5. Top 5 Productos Mes**

![KPI 5. Top 5 Productos Mes](quest5\img\DFTop5ProductosMes.png)

## **Resultados**

Con base a los DataFlows desarrollados se construyeron los pipelinas respectivos que ven en la siguiente im√°gen

![Pipelines](quest5\img\Pipelines.png)

A continuaci√≥n, se muestra la ejecuci√≥n de los pipelines

![Pipelines](quest5\img\Ejecuci√≥nPipelinesTriggers.png)

Y por √∫ltimo se puede ver los datos almacenados en el DataLake

![DataLake](quest5\img\DataLake.png)

---

# **Quest 7: Arquitectura**

En la empresa gaseosas SA est√°n trabajando en una soluci√≥n anal√≠tica que sea capaz de procesar miles de datos de las ventas donde se describen comportamiento de compra y an√°lisis previos hechos por vendedores a clientes con gran volumen de compra, de forma r√°pida y confiable mediante el uso de tecnolog√≠as Big Data de anal√≠tica, para entrenar un modelo que sea capaz de identificar los patrones de estas ventas y compararlos en tiempo real con los patrones de datos capturados de manera streaming por dispositivos implantados puntos de venta, para controlar tempranamente y evitar el desabastecimiento.

Tu tarea es realizar un correcto dise√±o de la arquitectura para la soluci√≥n anal√≠tica que podr√≠a soportar estos requerimientos. (Ilustra tu dise√±o y da una breve explicaci√≥n de su funcionamiento), es importa definir el gobierno de datos y modelos de acuerdo a los perfiles

## **Soluci√≥n**

En una etapa inicial de exploraci√≥n contempl√© la utilizaci√≥n de productos SaaS y PaaS tanao de AWS como Azure, sin embargo, debido a la facilidad de conocimeinto de los productos de Azure, finalmente se opt√≥ por esta propuesta.

![Im√°genDram√°tica](quest7\img\ImagenDram√°tica.jpeg)

Finalmente se decidi√≥ presentar la siguiente arquitectura final a muy alto nivel (poco detalle), en esta arquitectura no se contemplan arquitecturas de red, administraci√≥n de recursos, monitoreo, etc.

![Im√°genDram√°tica](quest7\img\ArquetecturaGaseosas.png)


### **Explicaci√≥n resumida de la arquitectura**

La arquitectura propuesta para Gaseosas S.A. permite procesar miles de datos de ventas y se√±ales en tiempo real provenientes de dispositivos instalados en los puntos de venta. El objetivo es anticipar comportamientos de consumo y evitar el desabastecimiento.

Para lograrlo, se separan dos flujos principales:

* **Dispositivos IoT**, que llegan desde dispositivos en tiendas, camiones, fabricas etc.
* **Uuarios**, como ventas hist√≥ricas y an√°lisis hechos por los vendedores.

Ambos flujos entran a una infraestructura flexible con balanceadores de carga y m√°quinas que pueden escalar seg√∫n la demanda, lo que asegura que el sistema se mantenga disponible aun cuando aumenta la cantidad de datos.

Toda la informaci√≥n llega a un almacenamiento central donde luego es procesada mediante herramientas de orquestaci√≥n y an√°lisis. Aqu√≠ se organizan los datos en capas (Bronze, Silver y Gold), lo que permite tenerlos primero en bruto, luego limpios y finalmente listos para an√°lisis y creaci√≥n de modelos de inteligencia artificial. Sobre esta base se entrenan modelos predictivos capaces de identificar patrones de compra y compararlos con los datos que llegan en tiempo real desde los dispositivos.

Finalmente, las capas procesadas alimentan reportes en Power BI, donde los equipos de ventas, log√≠stica y gerencia pueden visualizar alertas, tendencias y comportamientos relevantes.

### **Gobierno de Datos y Modelos**

El gobierno de datos en esta arquitectura b√°sicamente se encarga de que toda la informaci√≥n que entra y sale del sistema est√© bien organizada, sea segura y pueda usarse sin problemas por las √°reas que la necesitan.

Primero, cuando los datos llegan desde los dispositivos IoT y desde los sistemas de ventas, pasan por diferentes capas (Bronze, Silver y Gold). Esto ayuda a que los datos se limpien, se ordenen y queden en un formato est√°ndar para que los equipos de anal√≠tica y machine learning puedan trabajar sin errores.

Tambi√©n se manejan permisos por perfiles. Por ejemplo, los equipos t√©cnicos pueden ver datos m√°s detallados, mientras que quienes solo usan reportes en Power BI acceden solo a informaci√≥n final. Esto evita riesgos y asegura que cada persona vea solo lo que le corresponde.

Adem√°s, se registra de d√≥nde viene cada dato y c√≥mo se transforma. Eso es √∫til para auditor√≠as y para saber qu√© est√° pasando si un reporte o un modelo muestra resultados extra√±os.

Finalmente, los modelos de machine learning tambi√©n se administran: se guarda qu√© versi√≥n est√° en uso, con qu√© datos se entren√≥ y cu√°ndo debe actualizarse. As√≠ se evita que el modelo se vuelva obsoleto o d√© predicciones equivocadas.

# üõ†Ô∏è **Herramientas Usadas**
| Componente | Descripci√≥n |
|-----------|-------------|
| Python | Procesamiento del CSV |
| Pandas | Transformaciones |
| VS Code / Jupyter / Azure Data Studio | Entorno de desarrollo |
| Git | Control de versiones |
| Azure | Plataforma en la nube |
| Draw.io | Plataforma sketching |
| LucidChart | Plataforma sketching |
---

## üìö Bibliograf√≠a

- Docker. *Docker Desktop Documentation*. Disponible en: https://docs.docker.com/desktop/
- Microsoft Learn. *Azure Data Factory ‚Äì Control Flow Expression Language Functions*. Disponible en: https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions
- Microsoft Learn. *Quickstart: Create a Data Factory*. Disponible en: https://learn.microsoft.com/es-es/azure/data-factory/quickstart-create-data-factory
